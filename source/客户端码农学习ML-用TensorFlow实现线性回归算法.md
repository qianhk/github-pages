客户端码农学习ML —— 用TensorFlow实现线性回归算法

## 线性回归（Linear Regression）

线性回归算法是机器学习、统计分析中重要的算法之一，也是常用的相对简单的算法。

给定由d个属性描述的点集X=(x<sub>1</sub>;x<sub>2</sub>;...;x<sub>d</sub>), 线性模型试图学得一个通过属性的线性组合来进行预测的函数，即&fnof;(x)=w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>d</sub>x<sub>d</sub> + b，知道w和b后就能确定模型。


我们在高中数学中已经学过只有一个属性x求待定系数的情况，即最小二乘法，一系列离散点通过最小二乘法即可确定一条回归直线&fnof;(x)=kx+b，我们现在用TensorFlow实现并体验下机器学习的思想。

### 准备数据

首先通过numpy生成一些模拟数据并有意随机偏移点(x<sub>i</sub>,y<sub>i</sub>)，然后通过代码去读取数据并更新欲求参数k、b，使得k、b越来越接近真实值，使得f(x<sub>i</sub>)≈y<sub>i</sub>，从而使得估算的值与真实值差别最小，通常采用平方差来衡量这个差距。

平方差对应了欧几里得距离，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。


```

```

### 模型训练

```

```

学习框架会使用梯度下降法去寻找一个最优解, 使得方差最小。

学习率是个很重要的参数，如果过小，算法收敛耗时很长，如果过大，可能结果不收敛或者直接NAN无法得到结果。

### 展示结果

```

```

## 其他回归算法

除了线性回归算法，还有其它好几种回归算法:

### 戴明回归算法

最小二乘线性回归算法是最小化到回归直线的竖直距离，只考虑y值，而戴明回归算法是最小化到回归直线垂直距离，同时考虑x值与y值。

具体算法修改下相减的损失函数即可，两者计算结果基本一致。

### lasso回归与岭回归算法

主要是在公式中增加正则项来限制斜率，lasso回归增加L1正则项，岭回归增加L2正则项。

### 弹性网络回归算法

综合lasso回归和岭回归的一种算法，在损失函数中同时增加L1和L2正则项。

### 逻辑回归算法

将线性回归转换成一个二值分类器，通过sigmoid函数将线性回归的输出缩放到0、1之间，判断目标是否属于某一类。


## 参考

http://studentdeng.github.io/blog/2014/07/28/machine-learning-tutorial/
